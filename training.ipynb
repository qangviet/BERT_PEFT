{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/qangviet/BERT_PEFT.git","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:43.222378Z","iopub.execute_input":"2024-05-26T13:49:43.222831Z","iopub.status.idle":"2024-05-26T13:49:44.684315Z","shell.execute_reply.started":"2024-05-26T13:49:43.222790Z","shell.execute_reply":"2024-05-26T13:49:44.683167Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'BERT_PEFT'...\nremote: Enumerating objects: 10, done.\u001b[K\nremote: Counting objects: 100% (10/10), done.\u001b[K\nremote: Compressing objects: 100% (8/8), done.\u001b[K\nremote: Total 10 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (10/10), 6.64 KiB | 2.21 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"%cd BERT_PEFT","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:44.686519Z","iopub.execute_input":"2024-05-26T13:49:44.686854Z","iopub.status.idle":"2024-05-26T13:49:44.694080Z","shell.execute_reply.started":"2024-05-26T13:49:44.686825Z","shell.execute_reply":"2024-05-26T13:49:44.693060Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/BERT_PEFT\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer\nimport tqdm\nfrom sklearn.metrics import accuracy_score,  f1_score\nfrom sklearn.model_selection import train_test_split\nfrom datasets import load_dataset\n\nfrom bert_model import BertForSequenceClassification as MyBert\nfrom lora import (\n    add_lora_layers,\n    merge_lora_layers,\n    freeze_model,\n    unfreeze_model,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:44.695535Z","iopub.execute_input":"2024-05-26T13:49:44.695840Z","iopub.status.idle":"2024-05-26T13:49:51.780428Z","shell.execute_reply.started":"2024-05-26T13:49:44.695817Z","shell.execute_reply":"2024-05-26T13:49:51.779706Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Data Preprocessing\n\nData: GLUE - SST-2","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset('glue', 'sst2')\ndataset = dataset['train']\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:51.782968Z","iopub.execute_input":"2024-05-26T13:49:51.783774Z","iopub.status.idle":"2024-05-26T13:49:59.652150Z","shell.execute_reply.started":"2024-05-26T13:49:51.783738Z","shell.execute_reply":"2024-05-26T13:49:59.651300Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67b3b3a40e964ee7b09731509d655d5e"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 10.7MB/s]\nDownloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 408kB/s]\nDownloading data: 100%|██████████| 148k/148k [00:00<00:00, 769kB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef247a051a224c26adacb9cb0f5b42f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82515549f0d742338d97521a77822671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c26c6f496d742be8a519dd7b8916b42"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['sentence', 'label', 'idx'],\n    num_rows: 67349\n})"},"metadata":{}}]},{"cell_type":"code","source":"sample_size = 15000\nsentences = dataset['sentence']\nlabels = dataset['label']\ncombined_data = list(zip(sentences, labels))\nrandom.shuffle(combined_data)\ncombined_data = combined_data[:sample_size]\ntrain_size = int(15000 * 0.9)\ntrain_dataset = combined_data[:train_size]\nval_dataset = combined_data[train_size:]\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:59.653277Z","iopub.execute_input":"2024-05-26T13:49:59.653559Z","iopub.status.idle":"2024-05-26T13:49:59.879879Z","shell.execute_reply.started":"2024-05-26T13:49:59.653533Z","shell.execute_reply":"2024-05-26T13:49:59.879179Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Bert Trainer","metadata":{}},{"cell_type":"code","source":"class BertTrainer:\n    \n    def __init__(self, model, tokenizer, train_dataloader, eval_dataloader=None,\n                 epochs=1, lr=3e-5):\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.model = model.to(self.device)\n        self.tokenizer = tokenizer\n        self.train_dataloader = train_dataloader\n        self.eval_dataloader = eval_dataloader\n        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, eps=1e-8)\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.eval_loss = float('inf')\n        self.epochs = epochs\n        self.epochs_best = 0\n    \n    def train(self, evaluate=False):\n        \n        for epoch in range(self.epochs):\n            self.iteration(epoch, self.train_dataloader)\n            if evaluate and self.eval_dataloader is not None:\n                self.iteration(epoch, self.eval_dataloader, train=False)\n    \n    def evaluate(self):\n        epoch=0\n        self.iteration(epoch, self.eval_dataloader, train=False)\n        \n    def iteration(self, epoch, data_loader, train=True):\n        loss_accumulated = 0.\n        preds_all = []\n        labels_all = []\n        \n        self.model.train() if train else self.model.eval()\n        mode = 'train' if train else 'eval'\n        \n        batch_iter = tqdm.tqdm(\n            enumerate(data_loader),\n            desc=f\"Epoch ({mode}) {epoch + 1} / {self.epochs}\",\n            total=len(data_loader),\n            bar_format=\"{l_bar}{r_bar}\"\n        )\n        \n        for i, batch in batch_iter:\n            \n            batch_t = self.tokenizer(\n                batch[0],\n                padding='max_length',\n                max_length=256,\n                truncation=True,\n                return_tensors='pt',\n            )\n            batch_t = {key: value.to(self.device) for key, value in batch_t.items()}\n            input_labels = batch[1].to(self.device)\n            \n            logits = self.model(\n                input_ids=batch_t[\"input_ids\"],\n                token_type_ids=batch_t[\"token_type_ids\"],\n                attention_mask=batch_t['attention_mask'],\n                \n            )\n            loss = self.loss_fn(logits, input_labels)\n            \n            if train:\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n            \n            preds = logits.argmax(dim=-1)\n            loss_accumulated += loss.item()\n            preds_all.append(preds.detach())\n            labels_all.append(input_labels.detach())\n        \n        preds_all = torch.cat(preds_all, dim=0).cpu()\n        labels_all = torch.cat(labels_all, dim=0).cpu()\n        \n        accuracy = accuracy_score(labels_all, preds_all)\n        f1 = f1_score(labels_all, preds_all, average='macro')\n        avg_loss_epoch = loss_accumulated / len(data_loader)\n        print(\"\")\n        print(\"  Avg Loss: {0:.4f}\".format(avg_loss_epoch))\n        print(\"  F1 Score: {0:.4f}\".format(f1))\n        print(\"  Accuracy: {0:.4f}\".format(accuracy))\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:59.881049Z","iopub.execute_input":"2024-05-26T13:49:59.881323Z","iopub.status.idle":"2024-05-26T13:49:59.897455Z","shell.execute_reply.started":"2024-05-26T13:49:59.881299Z","shell.execute_reply":"2024-05-26T13:49:59.896740Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Fine tuning","metadata":{}},{"cell_type":"markdown","source":"#### BERT-base","metadata":{}},{"cell_type":"code","source":"tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_base = MyBert.from_pretrained(\n    model_type='bert-base-uncased',\n    config_args= {\n        \"vocab_size\": 30522, 'n_classes': 2, \"max_seq_len\": 256\n    },\n    adaptive_weight_copy = True,\n)\nbert_base.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:49:59.898496Z","iopub.execute_input":"2024-05-26T13:49:59.898763Z","iopub.status.idle":"2024-05-26T13:50:05.648859Z","shell.execute_reply.started":"2024-05-26T13:49:59.898737Z","shell.execute_reply":"2024-05-26T13:50:05.647917Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1810ee3f731744c3a27b54b5c020c629"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a680956726849219bde30f20eeb5d1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1903b7798104af1a44819da98c89902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f699a6f49b244a719d94bbe73f46381d"}},"metadata":{}},{"name":"stdout","text":"Loading weights from pretrained model: bert-base-uncased\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b22a97d0a3945c9b118202e7d9d51a9"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768)\n      (position_embeddings): Embedding(256, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (gelu): GELU(approximate='none')\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (tanh): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"trainer_bert_base = BertTrainer(\n    bert_base,\n    tokenizer_base,\n    lr=5e-5,\n    epochs=4,\n    train_dataloader=train_dataloader,\n    eval_dataloader=val_dataloader,\n)\ntrainer_bert_base.train(evaluate=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T13:50:05.650080Z","iopub.execute_input":"2024-05-26T13:50:05.650714Z","iopub.status.idle":"2024-05-26T14:12:51.244945Z","shell.execute_reply.started":"2024-05-26T13:50:05.650677Z","shell.execute_reply":"2024-05-26T14:12:51.243956Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Epoch (train) 1 / 4: 100%|| 422/422 [05:27<00:00,  1.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2942\n  F1 Score: 0.8790\n  Accuracy: 0.8808\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 1 / 4: 100%|| 47/47 [00:12<00:00,  3.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2158\n  F1 Score: 0.9167\n  Accuracy: 0.9187\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 2 / 4: 100%|| 422/422 [05:28<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.1217\n  F1 Score: 0.9566\n  Accuracy: 0.9573\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 2 / 4: 100%|| 47/47 [00:12<00:00,  3.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2765\n  F1 Score: 0.9101\n  Accuracy: 0.9133\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 3 / 4: 100%|| 422/422 [05:28<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.0652\n  F1 Score: 0.9786\n  Accuracy: 0.9790\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 3 / 4: 100%|| 47/47 [00:12<00:00,  3.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2728\n  F1 Score: 0.9180\n  Accuracy: 0.9200\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 4 / 4: 100%|| 422/422 [05:28<00:00,  1.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.0427\n  F1 Score: 0.9858\n  Accuracy: 0.9861\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 4 / 4: 100%|| 47/47 [00:12<00:00,  3.63it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.3079\n  F1 Score: 0.9153\n  Accuracy: 0.9173\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### LoRA BERT-base\n\nrank = 8","metadata":{}},{"cell_type":"code","source":"tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_base = MyBert.from_pretrained(\n    model_type='bert-base-uncased',\n    config_args= {\n        \"vocab_size\": 30522, 'n_classes': 2, \"max_seq_len\": 256\n    },\n    adaptive_weight_copy = True,\n)\nadd_lora_layers(bert_base, r = 8, lora_alpha=16)\nfreeze_model(bert_base)\nbert_base.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:12:51.246557Z","iopub.execute_input":"2024-05-26T14:12:51.247014Z","iopub.status.idle":"2024-05-26T14:12:53.149331Z","shell.execute_reply.started":"2024-05-26T14:12:51.246976Z","shell.execute_reply":"2024-05-26T14:12:53.148338Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Loading weights from pretrained model: bert-base-uncased\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"n_params = 0\nn_trainable_params = 0\n\n# count the number of trainable parameters\nfor n, p in bert_base.named_parameters():\n    n_params += p.numel()\n    if p.requires_grad:\n        n_trainable_params += p.numel()\n\nprint(f\"Total parameters: {n_params}\")\nprint(f\"Trainable parameters: {n_trainable_params}\")\nprint(f\"Percentage trainable: {round(n_trainable_params / n_params * 100, 2)}%\")","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:12:53.152486Z","iopub.execute_input":"2024-05-26T14:12:53.153059Z","iopub.status.idle":"2024-05-26T14:12:53.160479Z","shell.execute_reply.started":"2024-05-26T14:12:53.153023Z","shell.execute_reply":"2024-05-26T14:12:53.159637Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Total parameters: 109582082\nTrainable parameters: 296450\nPercentage trainable: 0.27%\n","output_type":"stream"}]},{"cell_type":"code","source":"#bert base lora all r = 8\ntrainer_bert_base_lora = BertTrainer(\n    bert_base,\n    tokenizer_base,\n    lr=5e-4,\n    epochs=4,\n    train_dataloader=train_dataloader,\n    eval_dataloader=val_dataloader,\n)\n\ntrainer_bert_base_lora.train(evaluate=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-26T14:12:53.161677Z","iopub.execute_input":"2024-05-26T14:12:53.161993Z","iopub.status.idle":"2024-05-26T14:29:30.466178Z","shell.execute_reply.started":"2024-05-26T14:12:53.161955Z","shell.execute_reply":"2024-05-26T14:29:30.465209Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Epoch (train) 1 / 4: 100%|| 422/422 [03:55<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.3114\n  F1 Score: 0.8595\n  Accuracy: 0.8628\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 1 / 4: 100%|| 47/47 [00:13<00:00,  3.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2542\n  F1 Score: 0.9038\n  Accuracy: 0.9060\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 2 / 4: 100%|| 422/422 [03:55<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.1903\n  F1 Score: 0.9244\n  Accuracy: 0.9257\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 2 / 4: 100%|| 47/47 [00:13<00:00,  3.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2213\n  F1 Score: 0.9132\n  Accuracy: 0.9147\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 3 / 4: 100%|| 422/422 [03:55<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.1202\n  F1 Score: 0.9549\n  Accuracy: 0.9557\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 3 / 4: 100%|| 47/47 [00:13<00:00,  3.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2292\n  F1 Score: 0.9249\n  Accuracy: 0.9267\n","output_type":"stream"},{"name":"stderr","text":"Epoch (train) 4 / 4: 100%|| 422/422 [03:56<00:00,  1.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.0715\n  F1 Score: 0.9776\n  Accuracy: 0.9780\n","output_type":"stream"},{"name":"stderr","text":"Epoch (eval) 4 / 4: 100%|| 47/47 [00:13<00:00,  3.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n  Avg Loss: 0.2846\n  F1 Score: 0.9201\n  Accuracy: 0.9220\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]}]}